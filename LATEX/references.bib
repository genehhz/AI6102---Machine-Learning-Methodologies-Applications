@misc{playground-series-s4e9,
    author = {Walter Reade and Ashley Chow},
    title = {Regression of Used Car Prices},
    year = {2024},
    howpublished = {\url{https://kaggle.com/competitions/playground-series-s4e9}},
    note = {Kaggle, https://kaggle.com/competitions/playground-series-s4e9}
}

@article{4a848dd1-54e3-3c3c-83c3-04977ded2e71,
     author = {Jerome H. Friedman},
     title = {Greedy Function Approximation: A Gradient Boosting Machine},
     year = {2001},
     journal = {The Annals of Statistics},
     number = {5},
     pages = {1189--1232},
     publisher = {Institute of Mathematical Statistics},
     urldate = {2024-11-21},
     volume = {29},
     ISSN = {00905364, 21688966},
     URL = {http://www.jstor.org/stable/2699986},
     abstract = {Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent "boosting" paradigm is developed for additive expansions based on any fitting criterion. Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such "TreeBoost" models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.}
}

@inproceedings{NIPS2017_6449f44a,
     author = {Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
     title = {LightGBM: A Highly Efficient Gradient Boosting Decision Tree},
     year = {2017},
     booktitle = {Advances in Neural Information Processing Systems},
     editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
     pages = {},
     publisher = {Curran Associates, Inc.},
     url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf},
     volume = {30},
}

@inproceedings{10.1145/2939672.2939785,
    author = {Chen, Tianqi and Guestrin, Carlos},
    title = {XGBoost: A Scalable Tree Boosting System},
    year = {2016},
    isbn = {9781450342322},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2939672.2939785},
    doi = {10.1145/2939672.2939785},
    abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
    booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
    pages = {785–794},
    numpages = {10},
    keywords = {large-scale machine learning},
    location = {San Francisco, California, USA},
    series = {KDD '16}
}

@inproceedings{10.5555/3327757.3327770,
    author = {Prokhorenkova, Liudmila and Gusev, Gleb and Vorobev, Aleksandr and Dorogush, Anna Veronika and Gulin, Andrey},
    title = {CatBoost: unbiased boosting with categorical features},
    year = {2018},
    booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    abstract = {This paper presents the key algorithmic techniques behind CatBoost, a new gradient boosting toolkit. Their combination leads to CatBoost outperforming other publicly available boosting implementations in terms of quality on a variety of datasets. Two critical algorithmic advances introduced in CatBoost are the implementation of ordered boosting, a permutation-driven alternative to the classic algorithm, and an innovative algorithm for processing categorical features. Both techniques were created to fight a prediction shift caused by a special kind of target leakage present in all currently existing implementations of gradient boosting algorithms. In this paper, we provide a detailed analysis of this problem and demonstrate that proposed algorithms solve it effectively, leading to excellent empirical results.},
    pages = {6639–6649},
    numpages = {11},
    location = {Montr\'{e}al, Canada},
    series = {NIPS'18}
}

@article{10.5555/1639537.1639542,
    author = {Popescu, Marius-Constantin and Balas, Valentina E. and Perescu-Popescu, Liliana and Mastorakis, Nikos},
    title = {Multilayer perceptron and neural networks},
    year = {2009},
    issue_date = {July 2009},
    publisher = {World Scientific and Engineering Academy and Society (WSEAS)},
    address = {Stevens Point, Wisconsin, USA},
    volume = {8},
    number = {7},
    issn = {1109-2734},
    abstract = {The attempts for solving linear inseparable problems have led to different variations on the number of layers of neurons and activation functions used. The backpropagation algorithm is the most known and used supervised learning algorithm. Also called the generalized delta algorithm because it expands the training way of the adaline network, it is based on minimizing the difference between the desired output and the actual output, through the downward gradient method (the gradient tells us how a function varies in different directions). Training a multilayer perceptron is often quite slow, requiring thousands or tens of thousands of epochs for complex problems. The best known methods to accelerate learning are: the momentum method and applying a variable learning rate. The paper presents the possibility to control the induction driving using neural systems.},
    journal = {WSEAS Trans. Cir. and Sys.},
    month = jul,
    pages = {579–588},
    numpages = {10},
    keywords = {backpropagation algorithm, gradient method, induction driving, multilayer perceptron}
}

@inproceedings{8947658,
  author={Yang, Feng-Jen},
  booktitle={2018 International Conference on Computational Science and Computational Intelligence (CSCI)}, 
  title={An Implementation of Naive Bayes Classifier}, 
  year={2018},
  volume={},
  number={},
  pages={301-306},
  keywords={Probabilistic logic;Bayes methods;Tools;Python;Machine learning;Data mining;Engines;Naive Bayes Classifier, Probabilistic Classification, Bayesian Theory},
  doi={10.1109/CSCI46756.2018.00065}}


@misc{fujimori2024prediction,
  author = {Kazuhiro Fujimori},
  title = {Prediction by Deep Learning Regression Model},
  year = {2024},
  url = {https://www.kaggle.com/code/kazuhirofujimori/prediction-by-deeplearning-regression-model},
  note = {Kaggle, https://www.kaggle.com/code/kazuhirofujimori/prediction-by-deeplearning-regression-model}
}

@misc{danishyousufdeeplearning,
  author = {Danish Yousuf},
  title = {Using Neural Network For Car Price Prediction},
  year = {2024},
  url = {https://www.kaggle.com/code/kazuhirofujimori/prediction-by-deeplearning-regression-model},
  note = {Kaggle, https://www.kaggle.com/code/kazuhirofujimori/prediction-by-deeplearning-regression-model}
}

@article{wong2019reliable,
  title={Reliable accuracy estimates from k-fold cross validation},
  author={Wong, Tzu-Tsung and Yeh, Po-Yang},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={32},
  number={8},
  pages={1586--1594},
  year={2019},
  publisher={IEEE}
}

@article{Tofallis_2015,
   title={A better measure of relative prediction accuracy for model selection and model estimation},
   volume={66},
   ISSN={1476-9360},
   url={http://dx.doi.org/10.1057/jors.2014.103},
   DOI={10.1057/jors.2014.103},
   number={8},
   journal={Journal of the Operational Research Society},
   publisher={Informa UK Limited},
   author={Tofallis, Chris},
   year={2015},
   month=aug, pages={1352–1362} }
